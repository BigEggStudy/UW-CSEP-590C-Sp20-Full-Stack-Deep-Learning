{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "many-to-one-rnn-example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK0-CfHgm-Nm",
        "colab_type": "text"
      },
      "source": [
        "# UW FSDL Spring 2020 - Many-to-one RNN example\n",
        "\n",
        "Super simple example of predicting sentiment from text, using the IMDB Reviews dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DpvluhienMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6NtyqaPgQzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download IMDB reviews dataset\n",
        "# https://www.tensorflow.org/datasets/catalog/imdb_reviews\n",
        "\n",
        "dataset = tfds.load('imdb_reviews')\n",
        "dataset_train, dataset_test = dataset['train'], dataset['test']\n",
        "\n",
        "for ex in dataset_train.take(4):\n",
        "  print(ex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV-DqXnegvc6",
        "colab_type": "code",
        "outputId": "d8b9a327-7953-4d38-a82d-0272142dcc1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Let's get just the texts as strings\n",
        "\n",
        "texts_train = [ex['text'].decode(\"utf-8\") for ex in tfds.as_numpy(dataset_train)]\n",
        "texts_test = [ex['text'].decode(\"utf-8\") for ex in tfds.as_numpy(dataset_test)]\n",
        "\n",
        "texts_train[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrWqewMajnmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To convert texts to tokens, with each token represented by an integer, we need a tokenizer.\n",
        "# It will strip out punctuation, split up words, and convert to integers.\n",
        "# Let's limit the vocabulary to 10K most freqently used words in the training dataset.\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(texts_train)\n",
        "tokenizer.get_config()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJSEjKYplAqw",
        "colab_type": "code",
        "outputId": "2c79d0f9-9297-434f-c91a-4e0f02cf6574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Now we can convert texts to sequences of integers\n",
        "tokens_train = tokenizer.texts_to_sequences(texts_train)\n",
        "tokens_test = tokenizer.texts_to_sequences(texts_test)\n",
        "\n",
        "print(tokens_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[11, 13, 32, 424, 391, 17, 89, 27, 8, 31, 1365, 3584, 39, 485, 196, 23, 84, 153, 18, 11, 212, 328, 27, 65, 246, 214, 8, 476, 57, 65, 84, 113, 97, 21, 5674, 11, 1321, 642, 766, 11, 17, 6, 32, 399, 8169, 175, 2454, 415, 1, 88, 1230, 136, 68, 145, 51, 1, 7576, 68, 228, 65, 2932, 15, 2903, 1478, 4939, 2, 38, 3899, 116, 1583, 16, 3584, 13, 161, 18, 3, 1230, 916, 7916, 8, 3, 17, 12, 13, 4138, 4, 98, 144, 1213, 10, 241, 682, 12, 47, 23, 99, 37, 11, 7180, 5514, 37, 1365, 49, 400, 10, 97, 1196, 866, 140, 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmQkp0nwvg7h",
        "colab_type": "code",
        "outputId": "2120a4a4-b0ae-4851-f286-fe1c178b0ca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Pad each sequence to the same length. We'll use 90-th percentile length (this will truncate some sequences).\n",
        "\n",
        "max_length = int(np.percentile([len(tokens) for tokens in tokens_train], 90))\n",
        "print(max_length)\n",
        "\n",
        "tokens_train = tf.keras.preprocessing.sequence.pad_sequences(tokens_train, max_length)\n",
        "tokens_test = tf.keras.preprocessing.sequence.pad_sequences(tokens_test, max_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpQHW44TzFmP",
        "colab_type": "code",
        "outputId": "3aa88593-f96a-4c8e-fcbb-4e640ce857bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokens_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 435)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwmHfOzYnOjK",
        "colab_type": "code",
        "outputId": "1ff4f420-eca4-4961-d2e7-2ef38f14e97d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels_train = np.array([ex['label'] for ex in tfds.as_numpy(dataset_train)])\n",
        "labels_test = np.array([ex['label'] for ex in tfds.as_numpy(dataset_test)])\n",
        "\n",
        "labels_train[0], labels_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, (25000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z-68d_6nYh6",
        "colab_type": "code",
        "outputId": "545427bc-2cd5-4c5d-8b83-bf9f9a8ed4e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "EMBEDDING_DIM = 64\n",
        "LSTM_DIM = 128\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# Embedding layer converts sequences of integers (our tokens) to EMBEDDING_DIM-sized real-valued vectors\n",
        "model.add(tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM))#, input_length=max_length))\n",
        "\n",
        "# # LSTM processes the embedded vectors in sequence, and outputs an LSTM_DIM-sized vector at the end.\n",
        "model.add(tf.keras.layers.LSTM(LSTM_DIM))\n",
        "\n",
        "# # We convert that LSTM_DIM-sized vector to a single value between 0 and 1 with a sigmoid Dense layer\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          640000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 738,945\n",
            "Trainable params: 738,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBUXkXSHq2PN",
        "colab_type": "code",
        "outputId": "ec671111-1408-43b6-b076-ae8a86627f29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(\n",
        "    x=tokens_train,\n",
        "    y=labels_train,\n",
        "    batch_size=128,\n",
        "    validation_data=(tokens_test, labels_test),\n",
        "    epochs=6\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "196/196 [==============================] - 11s 58ms/step - loss: 0.2475 - accuracy: 0.9062 - val_loss: 0.3940 - val_accuracy: 0.8601\n",
            "Epoch 2/6\n",
            "196/196 [==============================] - 11s 56ms/step - loss: 0.1852 - accuracy: 0.9340 - val_loss: 0.3132 - val_accuracy: 0.8745\n",
            "Epoch 3/6\n",
            "196/196 [==============================] - 11s 57ms/step - loss: 0.1821 - accuracy: 0.9304 - val_loss: 0.3572 - val_accuracy: 0.8600\n",
            "Epoch 4/6\n",
            "196/196 [==============================] - 11s 57ms/step - loss: 0.1436 - accuracy: 0.9492 - val_loss: 0.3649 - val_accuracy: 0.8628\n",
            "Epoch 5/6\n",
            "196/196 [==============================] - 11s 57ms/step - loss: 0.1010 - accuracy: 0.9663 - val_loss: 0.4140 - val_accuracy: 0.8528\n",
            "Epoch 6/6\n",
            "196/196 [==============================] - 11s 57ms/step - loss: 0.1143 - accuracy: 0.9599 - val_loss: 0.4202 - val_accuracy: 0.8530\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f336ffe2f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4WTHZxeskIE",
        "colab_type": "text"
      },
      "source": [
        "A few ideas to try:\n",
        "\n",
        "- Train for longer!\n",
        "- Different values for VOCAB_SIZE, EMBEDDING_DIM, or LSTM_DIM\n",
        "- Stack multiple LSTMs (you will need to pass in `return_sequences=True` -- read the LSTM() docstring for info)\n",
        "- Make the LSTM(s) bidirectional (look up how to do it)\n",
        "- Different optimizer or learning rate (look up how to do it)"
      ]
    }
  ]
}